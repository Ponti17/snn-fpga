{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# snntorch - Tutorial 7\n",
    "\n",
    "Neuromorphic datasets with `tonic` + `snntorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spiking Neural Network\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "from tonic import DiskCachedDataset\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Misc\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tonic.datasets.NMNIST(save_to='./data', train=True)\n",
    "events, target = dataset[0]\n",
    "print(events)\n",
    "tonic.utils.plot_event_grid(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "# Denoise removes isolated, one-off events\n",
    "# time_window\n",
    "frame_transform = transforms.Compose([transforms.Denoise(filter_time=10000),\n",
    "                                      transforms.ToFrame(sensor_size=sensor_size,\n",
    "                                                         time_window=1000)\n",
    "                                     ])\n",
    "\n",
    "trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_trainset = DiskCachedDataset(trainset, cache_path='./cache/nmnist/train')\n",
    "cached_dataloader = DataLoader(cached_trainset)\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())\n",
    "\n",
    "def load_sample_batched():\n",
    "    events, target = next(iter(cached_dataloader))\n",
    "\n",
    "%timeit -o -r 10 load_sample_batched()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using Frames from Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tonic.transforms.Compose([torch.from_numpy,\n",
    "                                      torchvision.transforms.RandomRotation([-10,10])])\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, transform=transform, cache_path='./cache/nmnist/train')\n",
    "\n",
    "# no augmentations for the testset\n",
    "cached_testset = DiskCachedDataset(testset, cache_path='./cache/nmnist/test')\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
    "testloader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "event_tensor, target = next(iter(trainloader))\n",
    "print(event_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.atan()\n",
    "beta = 0.5\n",
    "\n",
    "#  Initialize Network\n",
    "net = nn.Sequential(nn.Conv2d(2, 12, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Conv2d(12, 32, 5),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(32*5*5, 10),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "                    ).to(device)\n",
    "\n",
    "def forward_pass(net, data):\n",
    "    spk_rec = []\n",
    "    utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "    for step in range(data.size(0)):  # data.size(0) = number of time steps\n",
    "        spk_out, mem_out = net(data[step])\n",
    "        spk_rec.append(spk_out)\n",
    "\n",
    "    return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "\n",
    "num_epochs = 1\n",
    "num_iters = 50\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(trainloader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec = forward_pass(net, data)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # training loop breaks after 50 iterations\n",
    "        if i == num_iters:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\")\n",
    "plt.plot(acc_hist)\n",
    "plt.title(\"Train Set Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_rec = forward_pass(net, data)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "fig, ax = plt.subplots(facecolor='w', figsize=(12, 7))\n",
    "labels=['0', '1', '2', '3', '4', '5', '6', '7', '8','9']\n",
    "print(f\"The target label is: {targets[idx]}\")\n",
    "\n",
    "#  Plot spike count histogram\n",
    "anim = splt.spike_count(spk_rec[:, idx].detach().cpu(), fig, ax, labels=labels,\n",
    "                        animate=True, interpolate=1)\n",
    "video = HTML(anim.to_html5_video())\n",
    "plt.close()\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
